from .base import ParamManager
import numpy as np


class HybridParam(ParamManager):

    def initialize_weights(self, num_classes):
        """ initialize weights for distribution if dist_weights and num_classes not set """
        assert isinstance(num_classes, int)
        if not hasattr(self, 'dist_weights'):
            self.num_classes = num_classes
            # self.dist_weights  = .9 + np.zeros(num_classes)
            self.dist_weights = [.9] * num_classes

    def dump(self):
        print("******************************* Ensemble Parameters: *******************************")
        print("dist_weight per class: {}".format(self.dist_weights))
        print("************************************************************************************")

    def encode(self):
        """ 
        Encode all parameters to a vector, usually used by some optimizers (bayesian optimizer, genetic algorithm)
        Return:
            code: A list that enumerate all parameters in a fixed order
            bounds: A list of tuples (Nones if no boundaries are required) that lists the corresponding boundaries, like: [(low1, hight), (low2, high2), ...]
        """
        # code = self.dist_weights.tolist()
        code = self.dist_weights
        bounds = [(0, 1)] * self.num_classes
        return code, bounds

    def decode(self, code):
        """
        Update parameters given code that generated by self.encode()[0]
        """
        assert self.check_code(code)
        # self.dist_weights = np.array(code).flatten()
        self.dist_weights = code